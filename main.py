from crawler import crawl_url
from summarizer import summarize_text

# ìˆ˜ì§‘ëœ URLë“¤ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_urls(file_path):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        file_path (str): URLì´ ì €ì¥ëœ text íŒŒì¼ ê²½ë¡œ

    Returns:
        list: ì¤„ë°”ê¿ˆì„ ì œê±°í•œ URL ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    with open(file_path, 'r') as file:
        urls = [line.strip() for line in file if line.strip()]
    return urls


def main():
    file_path = 'url.txt'
    urls = load_urls(file_path)
    print("âœ… Loaded URLs:")
    for url in urls:
        print(f" - {url}")
        
    print("\nğŸš€ í¬ë¡¤ë§ ì „ì²´ ë³¸ë¬¸ í™•ì¸:")
    
    for url in urls:
        print(f"\nğŸ”— Crawling: {url}")
        body_text, image_url = crawl_url(url)
        
        # ëŒ€í‘œ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶œë ¥
        print(f"ğŸ–¼ï¸  Image: {image_url if image_url else '(ì—†ìŒ)'}")

        # ğŸ”½ ì „ì²´ ë³¸ë¬¸ ì¶œë ¥
        print("ğŸ“„ Extracted Full Text:\n")
        print(body_text)
        print("\n" + "="*100 + "\n")
               
if __name__ == "__main__":
    main()