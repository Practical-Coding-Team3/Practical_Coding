from crawler import crawl_url
from summarizer import summarize_text_first, summarize_text_remain
import json
from typing import List, Dict, Optional


def load_urls(file_path: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        file_path (str): URLì´ ì €ì¥ëœ text íŒŒì¼ ê²½ë¡œ

    Returns:
        list[str]: ì¤„ë°”ê¿ˆì„ ì œê±°í•œ URL ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    with open(file_path, 'r') as file:
        urls = [line.strip() for line in file if line.strip()]
    return urls

def process_url(url: str, is_first: bool = False) -> Dict[str, str]:
    """
    URLì„ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•˜ëŠ” í•¨ìˆ˜

    Args:
        url (str): ì²˜ë¦¬í•  URL
        is_first (bool): ì²« ë²ˆì§¸ URLì¸ì§€ ì—¬ë¶€ (ìì„¸í•œ ìš”ì•½ ì‚¬ìš©)

    Returns:
        Dict[str, str]: ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ”— Processing URL: {url}")
    
    # Crawling
    metadata, body_text, image_url = crawl_url(url)
    
    # Summary
    if is_first:
        summary = summarize_text_first(body_text, metadata)
    else:
        summary = summarize_text_remain(body_text)
    
    return {
        'url': url,
        'metadata': metadata,
        'summary': summary,
        'image_url': image_url
    }
    
def save_results(results: List[Dict[str, str]], output_file: str = 'results.json'):
    """
    ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜

    Args:
        results (List[Dict[str, str]]): ì €ì¥í•  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸íŠ¸
        output_file (str, optional): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent = 2)

def main():
    """
    ë©”ì¸ í•¨ìˆ˜ : URL ëª©ë¡ì„ ì½ì–´ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•œ í›„ ê²°ê³¼ë¥¼ ì €ì¥
    """
    # URL load
    file_path = 'url.txt'
    urls = load_urls(file_path)
    print("âœ… Loaded URLs:")
    for url in urls:
        print(f" - {url}")
        
    # ê° URL ì²˜ë¦¬
    results = []
    for i, url in enumerate(urls):
        result = process_url(url, is_first = (i == 0))
        results.append(result)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ Summary:")
        print(result['summary'])
        if result['image_url']:
            print(f"\nğŸ–¼ï¸  Image URL: {result['image_url']}")
        print("\n" + "="*100)
        
    # ê²°ê³¼ ì €ì¥
    save_results(results)
    print("\nâœ… Results saved to results.json")
    
               
if __name__ == "__main__":
    main()