from crawler import crawl_url

# ìˆ˜ì§‘ëœ URLë“¤ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_urls(file_path):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        file_path (str): URLì´ ì €ì¥ëœ text íŒŒì¼ ê²½ë¡œ

    Returns:
        list: ì¤„ë°”ê¿ˆì„ ì œê±°í•œ URL ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    with open(file_path, 'r') as file:
        urls = [line.strip() for line in file if line.strip()]
    return urls


def main():
    file_path = 'url.txt'
    urls = load_urls(file_path)
    print("URL list : ", urls)
    
    # crawling test -> ì¼ë‹¨ í¬ë¡¤ë§í•œ ë°ì´í„° ì¤‘ ì•ì˜ 300 ë‹¨ì–´ë§Œ
    for url in urls:
        print(f"\nğŸ”— Crawling: {url}")
        text = crawl_url(url)
        print(f"ğŸ“„ Extracted text (first 300 chars):\n{text[:300]}\n")
    
    
if __name__ == "__main__":
    main()